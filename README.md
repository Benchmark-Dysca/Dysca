<p align="center">
    <img src="figure/icon0.webp" width="150" style="margin-bottom: 0.2;"/>
<p>
<h2 align="center"> üé®Dysca: A Dynamic and Scalable Benchmark for Evaluating Perception Ability of LVLMs</h2>


<h4 align="center"> üéâIf you like our project, please give us a star ‚≠ê on GitHub for latest update.  </h4>


## Overviewüîç
<div>
    <img src="./figure/dysca_framework.svg" width="96%" height="96%">
</div>

<p align="center">Figure 1. Overview of the automatic pipeline in Dysca for generating VQAs, cleaning VQAs and evaluating LVLMs.</p>

<br> </br>
<div align=center>
  <img src="./figure/subtasks.svg" width="40%" height="40%">
</div>

<p align="center">Figure 2. The available subtasks of our Dysca.</p>
<br> </br>

**_Abstract -_** The remarkable advances of the Large Vision-Language Models (LVLMs) motivate the requirement to evaluate them. Among various evaluation aspects, perception is considered the most fundamental capability, and many benchmarks make efforts to assess that. However, most benchmarks conduct questions by selecting images from previous benchmarks rather than using in the wild images, resulting in a potential data leakage. Besides, these benchmarks merely focus on evaluating LVLMs on the realistic style images and clean scenarios, leaving the multi-stylized images and complex scenarios unexplored. In response to these challenges, we propose Dysca, a dynamic and scalable benchmark for evaluating LVLMs by leveraging synthesis images. 
Specifically, we leverage Stable Diffusion and design a rule-based method to dynamically generate novel images, questions and corresponding answers. 
We consider 50 kinds of image styles and evaluate the perception capability in 20 subtasks. Moreover, we conduct evaluations under 4 scenarios (i.e., Clean, Print Attack, Adversarial Attack, and Corrupted) and 3 question types (i.e., Multi-choice, True-or-false, and Free-form). A total of 11 advanced LVLMs are evaluated on Dysca, revealing the drawbacks of current LVLMs and demonstrating the effectiveness on evaluating LVLMs by using synthesis images. Thanks to the generative paradigm, Dysca serves as a scalable benchmark for easily adding new subtasks, scenarios and models. 


## Examples of Dyscaüì∏
Here are some examples of the images, prompts, questions and ground truth answers of our Dysca. These images are generated by the Stable Diffusion XL model.

<div>
  <img src="./figure/face_example.svg" width="50%" height="50%">
</div>
<div>
  <img src="./figure/animal_example.svg" width="50%" height="50%">
</div>


## Evaluation ResultsüèÜ


## Related projectsüîó
- [BLIP-2](https://github.com/salesforce/LAVIS/tree/main/projects/blip2)
- [InstructBLIP](https://github.com/salesforce/LAVIS/blob/main/projects/instructblip)
- [LLaVA-1.5](https://github.com/haotian-liu/LLaVA)
- [miniGPT4](https://github.com/Vision-CAIR/MiniGPT-4)
- [Otter](https://github.com/Vision-CAIR/MiniGPT-4)
- [Qwen-VL](https://github.com/QwenLM/Qwen-VL)
- [Shikra](https://github.com/shikras/shikra)
- [InternLM-XComposer](https://github.com/InternLM/InternLM-XComposer)



